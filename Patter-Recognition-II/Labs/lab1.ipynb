{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = load_digits()\n",
    "x,test_x,y,test_y = train_test_split(mnist.data,mnist.target,test_size=0.25,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.8711111111111111\n"
     ]
    }
   ],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 计算每个类的先验概率和保存为log值\n",
    "        num_classes = len(np.unique(y))\n",
    "        self.class_log_prior_ = np.log(np.bincount(y) / len(y))\n",
    "        \n",
    "        # 初始化存储特征条件概率的数组\n",
    "        self.feature_log_prob_ = np.zeros((num_classes, X.shape[1]))\n",
    "        \n",
    "        # 计算每个类中每个特征的条件概率\n",
    "        for c in np.unique(y):\n",
    "            X_c = X[y == c]\n",
    "            self.feature_log_prob_[c, :] = np.log((X_c.sum(axis=0) + 88) / (np.sum(X_c.sum(axis=0) -111)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 利用Bayes公式计算每个样本属于每个类的概率\n",
    "        log_prob = self.class_log_prior_ + X @ self.feature_log_prob_.T\n",
    "        return np.argmax(log_prob, axis=1)\n",
    "\n",
    "# 创建并训练模型\n",
    "model = MultinomialNaiveBayes()\n",
    "model.fit(x, y)\n",
    "\n",
    "# 进行预测\n",
    "z = model.predict(test_x)\n",
    "\n",
    "# 计算准确率\n",
    "print('准确率：', np.sum(z == test_y) / len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **先验概率的计算**\n",
    "\n",
    "    公式：$ P(y) $\n",
    "    ```python\n",
    "    self.class_log_prior_ = np.log(np.bincount(y) / len(y))\n",
    "    ```\n",
    "    这一行计算每个类$ y $的先验概率，并将结果保存为对数值。`np.bincount(y)`计算每个类在数据集中出现的次数，然后除以总样本数得到每个类的先验概率。\n",
    "\n",
    "2. **计算特征的条件概率**\n",
    "\n",
    "    公式：$ P(x_i | y) = \\frac{{\\text{Count}(x_i, y) + 1}}{{\\text{Count}(y) + |V|}} $\n",
    "    ```python\n",
    "    for c in np.unique(y):\n",
    "        X_c = x[y == c]\n",
    "        self.feature_log_prob_[c, :] = np.log((X_c.sum(axis=0) + 1) / (np.sum(X_c.sum(axis=0) + 1)))\n",
    "    ```\n",
    "    这一部分在循环中对每个类$ c $分别计算所有特征$ x_i $的条件概率。注意，这里使用了Laplace平滑（加1平滑）。\n",
    "\n",
    "### 预测\n",
    "\n",
    "1. **计算后验概率**\n",
    "\n",
    "    公式：$\\hat{y} = \\arg\\max_y \\left[ \\log P(y) + \\sum_{i=1}^{n} \\log P(x_i | y) \\right]$\n",
    "    ```python\n",
    "    log_prob = self.class_log_prior_ + X @ self.feature_log_prob_.T\n",
    "    ```\n",
    "    这一行使用先验概率的对数（`self.class_log_prior_`）和特征条件概率的对数（`self.feature_log_prob_`）来计算后验概率的对数。这里使用矩阵乘法简化了求和操作。\n",
    "\n",
    "2. **选择最高后验概率的类作为预测**\n",
    "\n",
    "    ```python\n",
    "    return np.argmax(log_prob, axis=1)\n",
    "    ```\n",
    "    这里我们简单地选择具有最高后验概率的类作为每个样本的预测标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新增超参：拉普拉斯平滑\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
