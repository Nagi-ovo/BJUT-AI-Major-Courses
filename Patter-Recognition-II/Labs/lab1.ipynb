{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/datasci/lib/python3.9/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载MNIST数据集\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
    "\n",
    "# 二值化\n",
    "X = (X > 127.5).astype(np.uint8)\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.8289142857142857\n"
     ]
    }
   ],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, alpha=1.0, V=None):\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "        self.alpha = alpha  # 平滑参数\n",
    "        self.V = V \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_classes = len(np.unique(y))\n",
    "        self.class_log_prior_ = np.log(np.bincount(y, minlength=num_classes) / len(y))\n",
    "        \n",
    "        self.feature_log_prob_ = np.zeros((num_classes, X.shape[1]))\n",
    "        \n",
    "        if self.V is None:  # 处理V为None的情况\n",
    "            self.V = X.shape[1]\n",
    "\n",
    "        for c in np.unique(y):\n",
    "            X_c = X[y == c]\n",
    "            self.feature_log_prob_[c, :] = np.log((X_c.sum(axis=0) + self.alpha) / (np.sum(X_c.sum(axis=0)) + self.V))\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_prob = X @ self.feature_log_prob_.T + self.class_log_prior_\n",
    "        return np.argmax(log_prob, axis=1)\n",
    "\n",
    "# 创建并训练模型\n",
    "model = MultinomialNaiveBayes(0.01, 5)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 进行预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "print('Accuracy : ', np.mean(y_pred == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳的alpha值为: 0.02\n",
      "最佳的V值为: -10\n",
      "最高准确率为: 0.8290285714285714\n"
     ]
    }
   ],
   "source": [
    "# 使用网格搜索（grid search）来自动查找更优的参数组合\n",
    "# 定义参数范围\n",
    "alpha_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.5, 1, 2, 5, 10, 50, 80, 100,1000]\n",
    "V_values = [None, -10, -5, -1, -0.1, 0.1, 0.5, 1, 2, 5, 10, 100,1000, 5000, 10000]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_alpha = None\n",
    "best_V = None\n",
    "\n",
    "# 对每种参数组合进行测试\n",
    "for alpha in alpha_values:\n",
    "    for V in V_values:\n",
    "        model = MultinomialNaiveBayes(alpha, V)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_alpha = alpha\n",
    "            best_V = V\n",
    "\n",
    "print(f\"Best alpha is : {best_alpha}\")\n",
    "print(f\"Best V is : {best_V}\")\n",
    "print(f\"Best Accuray is : {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **先验概率的计算**\n",
    "\n",
    "    公式：$ P(y) $\n",
    "    ```python\n",
    "    self.class_log_prior_ = np.log(np.bincount(y) / len(y))\n",
    "    ```\n",
    "    这一行计算每个类$ y $的先验概率，并将结果保存为对数值。`np.bincount(y)`计算每个类在数据集中出现的次数，然后除以总样本数得到每个类的先验概率。\n",
    "\n",
    "2. **计算特征的条件概率**\n",
    "\n",
    "    公式：$ P(x_i | y) = \\frac{{\\text{Count}(x_i, y) + 1}}{{\\text{Count}(y) + |V|}} $\n",
    "    ```python\n",
    "    for c in np.unique(y):\n",
    "        X_c = x[y == c]\n",
    "        self.feature_log_prob_[c, :] = np.log((X_c.sum(axis=0) + 1) / (np.sum(X_c.sum(axis=0) + 1)))\n",
    "    ```\n",
    "    这一部分在循环中对每个类$ c $分别计算所有特征$ x_i $的条件概率。注意，这里使用了Laplace平滑（加1平滑）。\n",
    "\n",
    "### 预测\n",
    "\n",
    "1. **计算后验概率**\n",
    "\n",
    "    公式：$\\hat{y} = \\arg\\max_y \\left[ \\log P(y) + \\sum_{i=1}^{n} \\log P(x_i | y) \\right]$\n",
    "    ```python\n",
    "    log_prob = self.class_log_prior_ + X @ self.feature_log_prob_.T\n",
    "    ```\n",
    "    这一行使用先验概率的对数（`self.class_log_prior_`）和特征条件概率的对数（`self.feature_log_prob_`）来计算后验概率的对数。这里使用矩阵乘法简化了求和操作。\n",
    "\n",
    "2. **选择最高后验概率的类作为预测**\n",
    "\n",
    "    ```python\n",
    "    return np.argmax(log_prob, axis=1)\n",
    "    ```\n",
    "    这里简单地选择具有最高后验概率的类作为每个样本的预测标签。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
