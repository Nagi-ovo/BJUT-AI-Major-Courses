**实验内容**：
使用函数 $f(x) = ||x-y||^2_2$，其中 $x=[x_1,x_2]^T$，$y=[3.3,2.001]^T$。以初始点 $x_0=[0,0]^T$ 为起点，应用梯度下降公式进行搜索。

**实验步骤**：
1. 计算目标函数的梯度。

$f(x) = ||x-y||_2^2 = (x_1 - y_1)^2 + (x_2 - y_2)^2$ 。

关于 $x_1$的偏导数是：$\frac{\partial f}{\partial x_1} = 2(x_1 - y_1)$，关于$x_2$的偏导数是：$\frac{\partial f}{\partial x_2} = 2(x_2 - y_2)$。

因此，梯度是： $\nabla f(x) = \begin{bmatrix} 2(x_1 - y_1) \\ 2(x_2 - y_2) \end{bmatrix}$。

使用下面的梯度下降公式来迭代更新 $x$：$x^{(t+1)} = x^{(t)} - \lambda \nabla f(x^{(t)})$   (式中的$\lambda$是学习率)。

2. 选择不同的学习率进行梯度下降，并记录每一次的迭代值和最终结果。
Python代码实现：
```python
import numpy as np

def gradient(x, y):
    """计算函数的梯度"""
    return 2 * (x - y)

def gradient_descent(y, learning_rate, max_iterations=1000, tol=1e-6):
    """
    使用梯度下降法找到函数的最小值。
    参数：
    - y: 目标向量
    - learning_rate: 学习率
    - max_iterations: 最大迭代次数
    - tol: 容忍度，当梯度的大小小于这个值时停止迭代
    返回：
    - x: 最优解
    - history: 每次迭代的解的历史记录
    """
    
    # 初始化 x 为 [0.0, 0.0]
    x = np.array([0.0, 0.0])
    history = [x.copy()]
    # 进行梯度下降
    for i in range(max_iterations):
        # 计算梯度
        grad = gradient(x, y)
        # 更新 x
        x = x - learning_rate * grad
```
```python
        # 保存本次迭代的 x
        history.append(x.copy())
        # 如果梯度的大小小于容忍度，则停止迭代
        if np.linalg.norm(grad) < tol:
            break
    return x, history

# 目标向量
y = np.array([3.3, 2.001])
# 对不同的学习率进行实验
for learning_rate in [0.01, 0.05, 0.1, 0.5, 1.0]:
    # 使用梯度下降法搜索最优解
    x_opt, history = gradient_descent(y, learning_rate)
    # 输出结果
    print(f"Learning rate: {learning_rate}")
    print(f"Optimal x: {x_opt}")
    print(f"Number of iterations: {len(history)}\n")
```
输出结果：
```text
Learning rate: 0.01 
Optimal x: [3.29999959 2.00099975] 
Number of iterations: 788 

Learning rate: 0.05 
Optimal x: [3.29999963 2.00099978] 
Number of iterations: 153 

Learning rate: 0.1 
Optimal x: [3.29999972 2.00099983] 
Number of iterations: 74 

Learning rate: 0.5 
Optimal x: [3.3 2.001] 
Number of iterations: 3 

Learning rate: 1.0 
Optimal x: [0. 0.] 
Number of iterations: 1001
```

![](storage%20bag/output.png)
**结论**：
1. 较小的学习率可能导致收敛非常缓慢。（$\lambda = 0.01$）
2. 适中的学习率可以提供较快的收敛速度，且保持较好的解。（$\lambda= 0.05$，$\lambda = 0.1$）
3. 较大的学习率可能导致算法快速收敛，但也存在跳过最优解的风险。（$\lambda = 0.5$）
4. 过大的学习率可能导致算法不收敛。（$\lambda = 1.0$）

**实验感想**：  
在实际应用梯度下降法时，应该根据具体问题适当调整，尝试从小到大不同的学习率，观察其收敛性和稳定性，以选择最适合的值。因为过大或过小的学习率都可能导致不良影响，如消耗过多时间和算力、不收敛等。